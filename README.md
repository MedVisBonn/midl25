# Adversarial Perturbations Improve Generalization of Confidence Prediction in Medical Image Segmentation

We introduce a straightforward adversarial training strategy that enhances the reliability of direct confidence prediction in medical image segmentation under domain shifts.

[MIDL 2025 Conference Paper](https://openreview.net/pdf?id=0BQ6JPGwZa)

## Table of Contents
- [Installation](#installation)
- [Usage](#usage)
- [Data & Models](#data--models)
- [Experiments & Results](#experiments--results)
- [Repository Structure](#repository-structure)
- [Citation & License](#citation--license)
- [Contact](#contact)

## Installation
- List dependencies and environment setup
- Command examples (pip, conda, etc.)

## Usage
- Instructions to run the code
- Example commands and expected outputs

## Data
We evaluate our approach using two datasets: the [SAML Dataset](https://liuquande.github.io/SAML/) and the [MNMS-2 Dataset](https://www.ub.edu/mnms-2/). To work with these datasets, please adapt the paths in the configuration file located at `src/configs/data` to match your local environment. Any pre-processing is handled by the respective classes in `src/dataset`.

## Experiments & Results
- Steps to replicate experiments
- Summary of results and metrics
- Instructions for reproducing plots/tables

## Citation & License
TBA

## Contact
- For questions, contact: lennartz (Ã¤t) cs.uni-bonn.de